# Gradient Descent Implementation ![Python](https://img.shields.io/badge/python-3.9+-blue) ![License](https://img.shields.io/badge/license-MIT-green)

#### A lightweight Python implementation of batch gradient descent for linear regression, designed for research and visualization purposes. This project allows you to train a linear model from scratch, track the evolution of weights and loss, and visualize convergence.
---

## Gradient Descent

Gradient Descent is a core optimization algorithm used to **minimize loss functions** by iteratively updating model parameters in the direction of the negative gradient. It is essential in **machine learning**, helping models like linear and logistic regression fit data effectively, and in **deep learning**, enabling neural networks with millions of parameters to learn complex patterns efficiently. Variants such as **batch, stochastic, and mini-batch gradient descent** make it flexible for datasets of all sizes. 

---

## Features of My Implementation

- ✅ Implements **Batch Gradient Descent** from scratch  
- ✅ Tracks **weights** and **loss history** at each iteration  
- ✅ **Visualizes**:
  - Regression line progression  
  - Loss convergence  
- ✅ Customizable **learning rate**, **number of iterations**, and **initial weights**  
- ✅ Pure Python + NumPy + Matplotlib, no heavy ML libraries


---

## Installation

```bash
# Clone the repository
git clone https://github.com/ArshiaMozafari/gradient-descent-implementation.git
cd gd-optimizer

# Install dependencies
pip install numpy matplotlib
